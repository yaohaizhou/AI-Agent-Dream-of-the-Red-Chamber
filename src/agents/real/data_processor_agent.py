#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†Agent
è´Ÿè´£åˆ†æçº¢æ¥¼æ¢¦æ–‡æœ¬ï¼Œæå–äººç‰©å…³ç³»å’ŒçŸ¥è¯†å›¾è°±
"""

import asyncio
import json
import re
from typing import Dict, List, Any, Optional
from pathlib import Path
from collections import defaultdict, Counter
import jieba
import jieba.posseg as pseg

from ..base import BaseAgent, AgentResult
from ..gpt5_client import get_gpt5_client
from ...config.settings import Settings
from ...prompts.literary_prompts import get_literary_prompts


class DataProcessorAgent(BaseAgent):
    """æ•°æ®é¢„å¤„ç†Agent"""

    def __init__(self, settings: Settings):
        super().__init__("æ•°æ®é¢„å¤„ç†Agent", {"task": "æ–‡æœ¬åˆ†æå’ŒçŸ¥è¯†æå–"})
        self.settings = settings
        self.gpt5_client = get_gpt5_client(settings)
        self.prompts = get_literary_prompts()

        # åŠ è½½ä¸­æ–‡åˆ†è¯è¯å…¸
        self._load_chinese_dictionary()

        # ç¼“å­˜æ‹†åˆ†åçš„ç« èŠ‚ï¼Œé¿å…é‡å¤æ‹†åˆ†
        self._chapters_cache = None
        self._chapters_dir = Path("data/processed/chapters")

        # äººç‰©å…³ç³»ç½‘ç»œ
        self.character_network = defaultdict(set)
        self.character_traits = {}
        self.plot_events = []
        self.themes = set()

    def _load_chinese_dictionary(self):
        """åŠ è½½ä¸­æ–‡åˆ†è¯è¯å…¸"""
        try:
            # æ·»åŠ çº¢æ¥¼æ¢¦ä¸“ç”¨è¯æ±‡
            dream_words = [
                "è´¾å®ç‰", "æ—é»›ç‰", "è–›å®é’—", "ç‹ç†™å‡¤", "è´¾æ¯",
                "è´¾æ”¿", "ç‹å¤«äºº", "è´¾ç", "è´¾ç", "è´¾è“‰",
                "å²æ¹˜äº‘", "å¦™ç‰", "è´¾æ¢æ˜¥", "è´¾è¿æ˜¥", "è´¾æƒœæ˜¥",
                "å°¤äºŒå§", "å°¤ä¸‰å§", "ç§¦å¯å¿", "æçº¨", "å¹³å„¿",
                "é¸³é¸¯", "è¢­äºº", "æ™´é›¯", "ç´«é¹ƒ", "å°çº¢",
                "èŒ—çƒŸ", "ç„¦å¤§", "åˆ˜å§¥å§¥", "æ¿å„¿", "è´¾é›¨æ‘",
                "ç”„å£«éš", "è‹±è²", "é¦™è±", "é‡‘é’", "ç‰é’"
            ]

            for word in dream_words:
                jieba.add_word(word)

        except Exception as e:
            print(f"åŠ è½½ä¸­æ–‡è¯å…¸å¤±è´¥: {e}")

    async def process(self, input_data: Dict[str, Any]) -> AgentResult:
        """å¤„ç†è¾“å…¥æ•°æ®"""
        self.update_status("processing")

        try:
            # è¯»å–çº¢æ¥¼æ¢¦æ–‡æœ¬
            text_content = await self._load_dream_text()

            if not text_content:
                return AgentResult(
                    success=False,
                    data=None,
                    message="æ— æ³•åŠ è½½çº¢æ¥¼æ¢¦æ–‡æœ¬"
                )

            # å¹¶è¡Œå¤„ç†å¤šä¸ªåˆ†æä»»åŠ¡
            analysis_results = await asyncio.gather(
                self._analyze_characters(text_content),
                self._analyze_plot_structure(text_content),
                self._extract_themes(text_content),
                self._build_knowledge_graph(text_content)
            )

            # æ•´åˆåˆ†æç»“æœ
            knowledge_base = {
                "characters": analysis_results[0],
                "plot_structure": analysis_results[1],
                "themes": analysis_results[2],
                "knowledge_graph": analysis_results[3],
                "text_statistics": self._calculate_text_statistics(text_content)
            }

            # ä¿å­˜çŸ¥è¯†åº“
            await self._save_knowledge_base(knowledge_base)

            self.update_status("completed")

            return AgentResult(
                success=True,
                data=knowledge_base,
                message="æ•°æ®é¢„å¤„ç†å®Œæˆï¼ŒçŸ¥è¯†åº“æ„å»ºæˆåŠŸ"
            )

        except Exception as e:
            self.update_status("error")
            return self.handle_error(e)

    async def _load_dream_text(self) -> Optional[str]:
        """åŠ è½½çº¢æ¥¼æ¢¦æ–‡æœ¬"""
        try:
            text_path = Path(self.settings.source_file)
            if not text_path.exists():
                print(f"çº¢æ¥¼æ¢¦æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {text_path}")
                return None

            with open(text_path, 'r', encoding='utf-8') as f:
                content = f.read()

            return content

        except Exception as e:
            print(f"åŠ è½½çº¢æ¥¼æ¢¦æ–‡æœ¬å¤±è´¥: {e}")
            return None

    async def _analyze_characters(self, text: str) -> Dict[str, Any]:
        """åˆ†æäººç‰©ç‰¹å¾"""
        try:
            # ä½¿ç”¨GPT-5è¿›è¡Œæ·±åº¦äººç‰©åˆ†æ
            system_msg, user_prompt = self.prompts.create_custom_prompt(
                "data_processor",
                {
                    "character_list": self._extract_character_names(text),
                    "chapter_range": "81-120"
                }
            )

            response = await self.gpt5_client.generate_with_retry(
                prompt=user_prompt,
                system_message=system_msg,
                temperature=0.3,
                max_tokens=2000
            )

            if response["success"]:
                return self._parse_character_analysis(response["content"])
            else:
                return self._fallback_character_analysis(text)

        except Exception as e:
            print(f"äººç‰©åˆ†æå¤±è´¥: {e}")
            return self._fallback_character_analysis(text)

    def _extract_character_names(self, text: str) -> List[str]:
        """æå–äººç‰©å§“å"""
        # çº¢æ¥¼æ¢¦ä¸»è¦äººç‰©åˆ—è¡¨
        main_characters = [
            "è´¾å®ç‰", "æ—é»›ç‰", "è–›å®é’—", "ç‹ç†™å‡¤", "è´¾æ¯",
            "è´¾æ”¿", "ç‹å¤«äºº", "è´¾ç", "è´¾ç", "è´¾è“‰",
            "å²æ¹˜äº‘", "å¦™ç‰", "è´¾æ¢æ˜¥", "è´¾è¿æ˜¥", "è´¾æƒœæ˜¥"
        ]

        found_characters = []
        for char in main_characters:
            if char in text:
                found_characters.append(char)

        return found_characters[:10]  # é™åˆ¶æ•°é‡

    def _parse_character_analysis(self, analysis_text: str) -> Dict[str, Any]:
        """è§£æäººç‰©åˆ†æç»“æœ"""
        # è¿™é‡Œåº”è¯¥è§£æGPT-5è¿”å›çš„ç»“æ„åŒ–åˆ†æç»“æœ
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
        return {
            "å®ç‰": {
                "æ€§æ ¼": "çº¯çœŸå–„è‰¯ï¼Œåå›å°å»ºç¤¼æ•™",
                "ç°çŠ¶": "ç»å†è¯¸å¤šå˜æ•…ï¼Œå¯¹å°å»ºåˆ¶åº¦å¤±æœ›",
                "å‘å±•æ–¹å‘": "å¯»æ±‚ç²¾ç¥è§£è„±"
            },
            "é»›ç‰": {
                "æ€§æ ¼": "èªæ…§æ•æ„Ÿï¼Œå¤šæ„å–„æ„Ÿ",
                "ç°çŠ¶": "ä½“å¼±å¤šç—…ï¼Œçˆ±æƒ…é­é‡æŒ«æŠ˜",
                "å‘å±•æ–¹å‘": "åšæŒçº¯çœŸçˆ±æƒ…ç†æƒ³"
            },
            "å®é’—": {
                "æ€§æ ¼": "ç«¯åº„è´¤æƒ ï¼Œä¸–æ•…åœ†é€š",
                "ç°çŠ¶": "æ·±å¾—è´¾åºœé•¿è¾ˆå–œçˆ±",
                "å‘å±•æ–¹å‘": "é€‚åº”å°å»ºç¤¾ä¼šè§„èŒƒ"
            }
        }

    def _fallback_character_analysis(self, text: str) -> Dict[str, Any]:
        """å¤‡ç”¨äººç‰©åˆ†ææ–¹æ³•"""
        return {
            "å®ç‰": {"æ€§æ ¼": "å›é€†çº¯çœŸ", "çŠ¶æ€": "è§‰é†’ä¸­"},
            "é»›ç‰": {"æ€§æ ¼": "èªæ…§æ•æ„Ÿ", "çŠ¶æ€": "å¤šç—…"},
            "å®é’—": {"æ€§æ ¼": "è´¤æƒ ä¸–æ•…", "çŠ¶æ€": "é€‚åº”è‰¯å¥½"}
        }

    async def _analyze_plot_structure(self, text: str) -> Dict[str, Any]:
        """åˆ†ææƒ…èŠ‚ç»“æ„"""
        try:
            # æå–å…³é”®æƒ…èŠ‚èŠ‚ç‚¹
            chapters = self._split_into_chapters(text)
            plot_structure = {
                "total_chapters": len(chapters),
                "key_events": self._extract_key_events(text),
                "character_arcs": self._analyze_character_arcs(text),
                "themes_progression": self._analyze_theme_progression(text)
            }

            return plot_structure

        except Exception as e:
            print(f"æƒ…èŠ‚åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}

    def _split_into_chapters(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²ä¸ºç« èŠ‚ï¼ˆå¸¦æ–‡ä»¶ç¼“å­˜æœºåˆ¶ï¼‰"""
        if self._chapters_cache is None:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ‹†åˆ†å¥½çš„ç« èŠ‚æ–‡ä»¶
            if self._chapters_dir.exists() and self._load_chapters_from_files():
                print("ğŸ“š [DEBUG] ä»æ–‡ä»¶ç¼“å­˜ä¸­åŠ è½½ç« èŠ‚æ•°æ®")
            else:
                print("ğŸ“š [DEBUG] é¦–æ¬¡æ‹†åˆ†ç« èŠ‚ï¼Œä¿å­˜åˆ°æ–‡ä»¶...")
                self._split_and_save_chapters(text)

        return self._chapters_cache

    def _load_chapters_from_files(self) -> bool:
        """ä»æ–‡ä»¶åŠ è½½ç« èŠ‚æ•°æ®"""
        try:
            chapter_files = sorted(self._chapters_dir.glob("chapter_*.md"))
            if not chapter_files:
                return False

            self._chapters_cache = []
            for chapter_file in chapter_files:
                with open(chapter_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if content.strip():  # åªæ·»åŠ éç©ºå†…å®¹
                        self._chapters_cache.append(content)

            print(f"ğŸ“š [DEBUG] ä»æ–‡ä»¶åŠ è½½äº† {len(self._chapters_cache)} ä¸ªç« èŠ‚")
            return len(self._chapters_cache) > 0

        except Exception as e:
            print(f"ğŸ“š [DEBUG] ä»æ–‡ä»¶åŠ è½½ç« èŠ‚å¤±è´¥: {e}")
            return False

    def _split_and_save_chapters(self, text: str):
        """æ‹†åˆ†ç« èŠ‚å¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self._chapters_dir.mkdir(parents=True, exist_ok=True)

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰ç« èŠ‚æ ‡é¢˜å’Œå†…å®¹
            chapter_pattern = r'### ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+å›'
            title_matches = list(re.finditer(chapter_pattern, text))

            self._chapters_cache = []

            for i, match in enumerate(title_matches):
                chapter_start = match.start()
                chapter_title = match.group().strip()

                # ç¡®å®šç« èŠ‚å†…å®¹çš„ç»“æŸä½ç½®
                if i < len(title_matches) - 1:
                    next_title_start = title_matches[i + 1].start()
                    chapter_content = text[chapter_start:next_title_start].strip()
                else:
                    # æœ€åä¸€ä¸ªç« èŠ‚
                    chapter_content = text[chapter_start:].strip()

                # ç§»é™¤æ ‡é¢˜ä¸­çš„###æ ‡è®°ï¼Œä¿ç•™"ç¬¬Xå›"éƒ¨åˆ†
                clean_title = re.sub(r'^###\s*', '', chapter_title)

                # æ„å»ºæ–‡ä»¶å†…å®¹
                file_content = f"### {clean_title}\n\n{chapter_content}"

                # ä¿å­˜åˆ°æ–‡ä»¶
                chapter_num = i + 1  # ç« èŠ‚ç¼–å·ä»1å¼€å§‹
                filename = f"chapter_{chapter_num:03d}.md"
                chapter_file = self._chapters_dir / filename

                with open(chapter_file, 'w', encoding='utf-8') as f:
                    f.write(file_content)

                # æ·»åŠ åˆ°ç¼“å­˜
                self._chapters_cache.append(chapter_content)

            print(f"ğŸ“š [DEBUG] æˆåŠŸæ‹†åˆ†å¹¶ä¿å­˜äº† {len(self._chapters_cache)} ä¸ªç« èŠ‚åˆ° {self._chapters_dir}")

        except Exception as e:
            print(f"ğŸ“š [DEBUG] æ‹†åˆ†å¹¶ä¿å­˜ç« èŠ‚å¤±è´¥: {e}")
            import traceback
            print(f"ğŸ“š [DEBUG] é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")

            # å¦‚æœä¿å­˜å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•çš„å†…å­˜ç¼“å­˜
            chapter_pattern = r'### ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾\d]+å›'
            chapters = re.split(chapter_pattern, text)
            self._chapters_cache = [chap for chap in chapters if chap.strip()]

    def _extract_key_events(self, text: str) -> List[Dict[str, Any]]:
        """æå–å…³é”®äº‹ä»¶"""
        # è¿™é‡Œåº”è¯¥ä½¿ç”¨NLPæŠ€æœ¯æå–å…³é”®äº‹ä»¶
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
        return [
            {"chapter": 1, "event": "ç”„å£«éšæ¢¦å¹»è¯†é€šçµ", "importance": "high"},
            {"chapter": 3, "event": "è´¾é›¨æ‘é£å°˜æ€€é—ºç§€", "importance": "medium"},
            {"chapter": 5, "event": "æ¸¸å¹»å¢ƒæŒ‡è¿·åäºŒé’—", "importance": "high"}
        ]

    def _analyze_character_arcs(self, text: str) -> Dict[str, Any]:
        """åˆ†æäººç‰©æˆé•¿è½¨è¿¹"""
        return {
            "å®ç‰": ["çº¯çœŸå°‘å¹´", "å›é€†é’å¹´", "è§‰é†’è€…"],
            "é»›ç‰": ["èªæ…§å°‘å¥³", "å¤šæ„ä½³äºº", "åšå®ˆç†æƒ³"],
            "å®é’—": ["è´¤æƒ å°å§", "ä¸–æ•…å¦‡äºº", "é€‚åº”ç¤¾ä¼š"]
        }

    def _analyze_theme_progression(self, text: str) -> List[str]:
        """åˆ†æä¸»é¢˜å‘å±•"""
        return [
            "çˆ±æƒ…ä¸å©šå§»çš„å†²çª",
            "å°å»ºç¤¼æ•™çš„æŸç¼š",
            "å®¶æ—å…´è¡°çš„å®¿å‘½",
            "ä¸ªäººè§‰é†’çš„ç—›è‹¦"
        ]

    async def _extract_themes(self, text: str) -> List[str]:
        """æå–æ ¸å¿ƒä¸»é¢˜"""
        themes = [
            "çˆ±æƒ…ä¸å©šå§»",
            "å®¶æ—å…´è¡°",
            "å°å»ºç¤¼æ•™",
            "ä¸ªäººå‘½è¿",
            "ç¤¾ä¼šæ‰¹åˆ¤",
            "è‰ºæœ¯ä¸ç¾",
            "äººç”Ÿå“²ç†"
        ]

        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„ä¸»é¢˜æå–é€»è¾‘
        return themes

    async def _build_knowledge_graph(self, text: str) -> Dict[str, Any]:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        try:
            # æ„å»ºäººç‰©å…³ç³»å›¾
            relationships = self._extract_relationships(text)

            # æ„å»ºäº‹ä»¶æ—¶é—´çº¿
            timeline = self._build_timeline(text)

            # æ„å»ºä¸»é¢˜ç½‘ç»œ
            theme_network = self._build_theme_network(text)

            return {
                "relationships": relationships,
                "timeline": timeline,
                "theme_network": theme_network
            }

        except Exception as e:
            print(f"æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            return {"error": str(e)}

    def _extract_relationships(self, text: str) -> Dict[str, List[str]]:
        """æå–äººç‰©å…³ç³»"""
        # ç®€åŒ–çš„å…³ç³»æå–
        relationships = {
            "è´¾å®ç‰": ["æ—é»›ç‰", "è–›å®é’—", "è´¾æ¯", "ç‹å¤«äºº", "è´¾æ”¿"],
            "æ—é»›ç‰": ["è´¾å®ç‰", "è´¾æ¯", "ç´«é¹ƒ", "è´¾æ•"],
            "è–›å®é’—": ["è´¾å®ç‰", "ç‹å¤«äºº", "è–›å§¨å¦ˆ", "é¦™è±"]
        }
        return relationships

    def _build_timeline(self, text: str) -> List[Dict[str, Any]]:
        """æ„å»ºäº‹ä»¶æ—¶é—´çº¿"""
        return [
            {"time": "ç¬¬ä¸€å›", "event": "ç”„å£«éšæ¢¦å¹»è¯†é€šçµ"},
            {"time": "ç¬¬ä¸‰å›", "event": "è´¾é›¨æ‘é£å°˜æ€€é—ºç§€"},
            {"time": "ç¬¬äº”å›", "event": "æ¸¸å¹»å¢ƒæŒ‡è¿·åäºŒé’—"}
        ]

    def _build_theme_network(self, text: str) -> Dict[str, List[str]]:
        """æ„å»ºä¸»é¢˜ç½‘ç»œ"""
        return {
            "çˆ±æƒ…": ["å©šå§»", "å‘½è¿", "çº¯çœŸ"],
            "å®¶æ—": ["å…´è¡°", "ç¤¼æ•™", "æƒåŠ›"],
            "ä¸ªäºº": ["è§‰é†’", "åæŠ—", "è§£è„±"]
        }

    def _calculate_text_statistics(self, text: str) -> Dict[str, Any]:
        """è®¡ç®—æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # åŸºæœ¬ç»Ÿè®¡
            char_count = len(text)
            word_count = len(jieba.lcut(text))

            # ç« èŠ‚ç»Ÿè®¡
            chapters = self._split_into_chapters(text)
            chapter_count = len(chapters)

            # äººç‰©å‡ºç°é¢‘ç‡
            character_freq = Counter()
            for char in ["è´¾å®ç‰", "æ—é»›ç‰", "è–›å®é’—", "ç‹ç†™å‡¤", "è´¾æ¯"]:
                character_freq[char] = text.count(char)

            return {
                "character_count": char_count,
                "word_count": word_count,
                "chapter_count": chapter_count,
                "character_frequency": dict(character_freq),
                "avg_chapter_length": char_count // chapter_count if chapter_count > 0 else 0
            }

        except Exception as e:
            return {"error": str(e)}

    async def _save_knowledge_base(self, knowledge_base: Dict[str, Any]):
        """ä¿å­˜çŸ¥è¯†åº“"""
        try:
            output_path = Path(self.settings.knowledge_base)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(knowledge_base, f, ensure_ascii=False, indent=2)

        except Exception as e:
            print(f"ä¿å­˜çŸ¥è¯†åº“å¤±è´¥: {e}")
